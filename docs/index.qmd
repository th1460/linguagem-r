---
title: "Linguagem R"
subtitle: "Além do #rstats"
author: "Thiago Pires"
format: 
  revealjs:
    theme: resources/scss/theme.scss
    footer: "[github.com/th1460/linguagem-r](https://github.com/th1460/linguagem-r/)"
    title-slide-attributes:
      data-background-image: resources/images/cover.gif
      data-background-size: cover
    width: 1600
    height: 900
---

## Thiago Pires

### Cientista de Dados (CIO/IBM)

:::: columns
::: {.column}
![](resources/images/avatar.jpeg){.rounded-corners .absolute left=5%}
:::
::: {.column}
<h4>Formação:</h4><br>
<ul>
<li>Estatística (UERJ)</li>
<li>MSc. Epidemiologia (ENSP/FIOCRUZ)</li>
<li>DSc. Engenharia Biomédica (COPPE/UFRJ)</li>
</ul>
:::
::::

## Linguagem R

:::: columns
::: {.column width="60%"}
<ul>
<li>A linguagem R foi derivada da linguagem S.</li>
<li>Foi lançada em 1993 pelos dois estatísticos Ross Ihaka e Robert Gentleman da Universidade de Auckland, Nova Zelândia</li>
<li>Atualmente na versão 4.2</li>
</ul>

![](resources/images/r-logo.png){.absolute width="15%" left=15% top="70%"}
:::

::: {.column width="40%"}
![](resources/images/ross-ihaka.jpeg){.absolute width="35%" left=60% top="1%"}
![](resources/images/robert-gentleman.jpeg){.absolute width="27%" left=60% top="45%"}
:::
::::

## Linguagem R

### Sintaxe

:::: columns
::: {.column width="50%"}
```{r}
#| echo: true

# x recebe um inteiro
x <- 4

# y recebe uma string
y <- "versão"

# Função que concatena
paste(y, x)

```

```{r}
#| echo: true

# Criando funções
f <- function(x) {
    x^2
}

f(-4:4)

# Utilizando pipe (versão 4.1+)
-4:4 |> f()
```
:::
::: {.column width="50%"}
```{r}
#| echo: true

# Trabalhando com dados
dados <- data.frame(x = c(4, 3, 5),
                    y = letters[3:5])
dados

# Criando variáveis
dados$z <- dados$x |> f()
dados

# Calculando medidas
dados$z |> mean()
```
:::
::::

## Linguagem R
### Sintaxe

:::: columns
::: {.column width="50%"}
```{r}
#| echo: true

# Criando listas
list(nome = c("Thiago", "Pires"), 
     idade = 37,
     skills = data.frame(skill = c("R", "Python"), 
                         anos = c(15, 3)))
```
:::
::: {.column width="50%"}
```{r}
#| echo: true

# Criando gráficos
x <- seq(-5, 5, .1)
y <- f(x)

plot(x, y, type = "l", col = "blue")
```
:::
::::

## [Tidyverse](https://www.tidyverse.org/) e [RStudio](https://www.rstudio.com/)

:::: columns
::: {.column width=50%}
![](resources/images/tidyverse.png)
:::
::: {.column width=50%}

> *`Tidyverse` é uma coleção de pacotes desenvolvidos pela RStudio e projetados para ciência de dados. Todos estes pacotes tentam compartilhar uma mesma sintaxe e estrutura de dados.*

:::
::::

## `dplyr::` utilizado na manipulação de dados

Média, desvio-padrão e N da idade, segundo sexo

```{r}
#| echo: true
#| message: false
#| output-location: column

library(dplyr)

titanic::titanic_train |> 
    filter(Survived == 0) |> 
    group_by(Sex) |> 
    summarise(`Média` = mean(Age, na.rm = TRUE),
              `Desvio-padrão` = sd(Age, na.rm = TRUE),
              N = n()) |> 
    mutate(across(c(2, 3), \(x) round(x, 1)))
```

Percentual de mortos, segundo sexo

```{r}
#| echo: true
#| message: false
#| output-location: column

titanic::titanic_train |>
    count(Survived, Sex) |>
    group_by(Sex) |>
    mutate(`%` = round(n/sum(n) * 100, 1)) |>
    filter(Survived == 0)
```

## `ggplot2::` utilizado na visualização de dados

Percentual de mortos e vivos, segundo sexo e classe

```{r}
#| echo: true
#| output-location: column
#| fig-height: 8
#| fig-width: 12

library(ggplot2)

titanic::titanic_train |> 
    ggplot() +
    aes(Sex, ..count../sum(..count..), 
        group = Survived, 
        fill = as.factor(Survived)) +
    geom_bar(position = "fill") +
    facet_grid(~Pclass) +
    scale_y_continuous(labels = scales::percent) +
    scale_fill_manual(values = c("#003f9a", "#5099f4"), 
                      labels = c("não", "sim")) +
    labs(x = "Sexo", y = "", fill = "Sobreviveu") +
    theme_light() + 
    theme(text = element_text(size = 24))
```

## `stringr::` e `forcats::`

```{r}
#| echo: true
#| output-location: column

titanic::titanic_train$Name[4]
```

```{r}
#| echo: true
#| output-location: column

titanic::titanic_train |> 
    dplyr::mutate(
        Title = Name |> 
            stringr::str_extract(
                "(?<=\\,\\s)(.*)(?=\\.)") |> 
            forcats::fct_lump(n = 4)) |>  
    ggplot() + 
    aes(Title, Age) +
    geom_boxplot() +
    theme(text = element_text(size = 24))
```

## `readr::`, `tidyr::` e `lubridate::` para mais manipulações

```{r}
#| echo: true
#| output-location: column

url <- "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"

covid19 <- 
    readr::read_csv(url) |> 
    dplyr::filter(`Country/Region` == "Brazil") |> 
    dplyr::select(- c(`Province/State`, Lat, Long))

covid19
```

```{r}
#| echo: true
#| output-location: column

covid19 <- 
    covid19 |> 
    tidyr::pivot_longer(!`Country/Region`, 
                        names_to = "date", 
                        values_to = "cumulate") |> 
    dplyr::group_by(`Country/Region`) |> 
    dplyr::mutate(date = lubridate::mdy(date),
                  value = cumulate - dplyr::lag(cumulate)) |> 
    dplyr::rename(country = `Country/Region`)

covid19 |> head(5)
```

## `ggplot2::` na visualização de casos de COVID 19

```{r}
#| echo: true
#| fig-height: 5
#| fig-width: 16

covid19 |> 
    dplyr::mutate(moving_average = zoo::rollmean(value, 7, align = "right", fill = NA)) |> 
    ggplot2::ggplot() +
    ggplot2::aes(date, moving_average) +
    ggplot2::geom_line() +
    ggplot2::theme_minimal() +
    ggplot2::labs(x = "Data", 
                  y = "Média móvel casos COVID 19") +
    theme(text = element_text(size = 22))
```

## `plotly::` gráficos dinâmicos

```{r}
#| echo: true
#| fig-height: 8
#| fig-width: 16

covid19 |> 
    dplyr::mutate(moving_average = zoo::rollmean(value, 7, align = "right", fill = NA)) |> 
    plotly::plot_ly(x = ~date, y = ~moving_average, 
                    type = "scatter", mode = "lines") |> 
    plotly::layout(xaxis = list(title = "Date"),
                   yaxis = list(title = "Média móvel casos COVID 19"),
                   font = list(size = 20), margin = list(t = 100)) |> 
    plotly::rangeslider() |> 
    widgetframe::frameWidget()
```

## `leaflet::` mapas dinâmicos

```{r}
#| echo: true
#| output-location: column

library(leaflet)

locale <- dplyr::tibble(label = "IBM Hortolândia",
                        lat = -22.8996401,
                        lng = -47.2032362,
                        logo = "https://www.ibm.com/brand/experience-guides/developer/8f4e3cc2b5d52354a6d43c8edba1e3c9/02_8-bar-reverse.svg")

leaflet(options = list(closePopupOnClick = FALSE)) |> 
    setView(lng = -46.9, 
            lat = -22.8, 
            zoom = 7) |> 
    addTiles() |> 
    addPopups(data = locale, 
              ~lng, 
              ~lat,
              options = popupOptions(closeButton = FALSE), 
              popup = ~paste("<img src=", 
                             logo, "width=100%> <br>", 
                             label))
```

## `leaflet::` rota do Titanic

```{r}
#| echo: true
#| output-location: column

events <- bind_rows(
    tibble(location = "Southampton (10-04-1912)", 
           lng = -1.4191, lat = 50.7894),
    tibble(location = "Cherbourg (10-04-1912)", 
           lng = -1.6109, lat = 49.6445),
    tibble(location = "Queenstown (11-04-1912)", 
           lng = -8.3211, lat = 51.8535),
    tibble(location = "Naufrágio (14-04-1912)", 
           lng = -49.9408, lat = 41.7258),
    tibble(location = "New York", 
           lng = -73.9655, lat = 40.6832))

leaflet() |> 
    setView(lng = -33.9, 
            lat = 46.8, 
            zoom = 3) |> 
    addTiles() |> 
    addCircleMarkers(data = events |> slice(1:3, 5), 
                     label = ~location,
                     color = c(rep("blue", 3), "green")) |> 
    addMarkers(data = events |> slice(4), 
               icon = list(
                   iconUrl = "resources/images/sinking-ship.jpeg", 
                   iconSize = c(50, 50)), 
               label = ~location) |> 
    addPolylines(data = events, ~lng, ~lat) |> 
    widgetframe::frameWidget()
```

## `DBI::`, `RJDBC::` e `dbplyr::` conectando com o DB2 na IBM Cloud

```{r}
#| echo: true
#| output-location: column

library(dbplyr)

# Ler variáveis de ambiente
readRenviron("../.Renviron") 

# Conexão com o DB2
drv <-
    RJDBC::JDBC("com.ibm.db2.jcc.DB2Driver", 
                "../jars/db2jcc4.jar")

host <- Sys.getenv("DB2_HOST")
user <- Sys.getenv("DB2_USER")
password <- Sys.getenv("DB2_PASSWORD")

uri <- 
    sprintf("jdbc:db2://%s/bludb:user=%s;password=%s;sslConnection=true;", 
            host, user, password)

db2 <-
    DBI::dbConnect(drv, uri)

# Enviando tabela para o DB2
DBI::dbWriteTable(db2, "COVID19", 
                  value = na.omit(covid19), 
                  overwrite = TRUE)

# Fazendo consulas no DB2
dplyr::tbl(db2, "COVID19") |> 
    dplyr::filter(dplyr::between(DATE, 
                                 "2022-06-07", 
                                 "2022-06-15")) |> 
    dplyr::select(COUNTRY, DATE, VALUE)
```

## `tidymodels::` aprendizado supervisionado

```{r}
#| echo: true

library(tidymodels)
set.seed(555)

titanic <- 
    titanic::titanic_train |> 
    dplyr::mutate(Pclass = factor(Pclass, labels = c("1st", "2nd", "3rd")),
                  Survived = factor(Survived, labels = c("não", "sim")))

# Separar 3/4 dos dados para treino 
data_split <- 
    initial_split(titanic, prop = 3/4)

train_data <- training(data_split)
test_data  <- testing(data_split)

# Ajuste do modelo
lr_mod <- 
    logistic_reg() |>  
    set_engine("glm")

lr_fit <- 
    lr_mod |>  
    fit(Survived ~ Sex + Pclass, data = train_data)

lr_fit |> broom::tidy()
```

## `tidymodels::` visualizando o modelo

```{r}
#| echo: true
#| output-location: column
#| fig-height: 6

newdata <- 
    expand.grid(Pclass = c("1st", "2nd", "3rd"),
                Sex = c("male", "female"))
pihat <- 
    (lr_fit |> predict(newdata, type = "prob")) |> 
    pull(.pred_sim)

newdata |> mutate(Pihat = pihat) |>  
    ggplot(aes(Sex, Pihat, 
               group = Pclass, 
               colour = Pclass)) + 
    geom_line() + geom_point() +
    labs(x = "Sex", y = expression(pi(Survived == sim)), 
         colour = "Ticket Class") +
    theme_minimal() +
    theme(text = element_text(size = 22))
```

## `tidymodels::` avaliando o modelo

```{r}
#| echo: true
#| output-location: column

measure <- function(data) {
    
    data |> 
        accuracy(truth = Survived, .pred_class) |>  
        
        bind_rows(
            data |>  
                f_meas(truth = Survived, .pred_class))
}

predict(lr_fit, test_data) |> 
    bind_cols(predict(lr_fit, 
                      test_data, type = "prob")) |> 
    bind_cols(test_data |>  select(Survived)) |> 
    measure()

```

## `pins::` versionando recursos no cloud object storage (COS) na IBM Cloud

Versionando o modelo selecionado para a predição da probabilidade de sobreviver ao desastre do titanic.

```{r}
#| echo: true

# Obtendo o modelo
saved_lr_fit <- tidypredict::parse_model(lr_fit)

# Conexão com o COS
board <- pins::board_s3(bucket = Sys.getenv("COS_BUCKET"),
                        region = Sys.getenv("COS_REGION"),
                        access_key = Sys.getenv("COS_ACCESS_KEY_ID"),
                        secret_access_key = Sys.getenv("COS_SECRET_ACCESS_KEY"),
                        endpoint = Sys.getenv("COS_ENDPOINT"))
```

```{r eval=FALSE}
#| echo: true

# Salvando o modelo
board |> pins::pin_write(saved_lr_fit, name = "titanic-model")
```

Lendo o modelo do COS e fazendo a predição

```{r}
#| echo: true

model <- board |> pins::pin_read("titanic-model")
input <- data.frame(Sex = "male", Pclass = "3rd")
pred <- tidypredict::tidypredict_to_column(input, model)

pred
```

## `plumber::` criando api

```{r eval=FALSE}
#| echo: true

# plumber.R

#* @apiTitle Prediction Survived in Titanic Disaster

#* @param sex Sex
#* @param pclass Class
#* @post /predict
function(sex, pclass) {
    
    board <- pins::board_s3(bucket = Sys.getenv("COS_BUCKET"),
                            region = Sys.getenv("COS_REGION"),
                            access_key = Sys.getenv("COS_ACCESS_KEY_ID"),
                            secret_access_key = Sys.getenv("COS_SECRET_ACCESS_KEY"),
                            endpoint = Sys.getenv("COS_ENDPOINT"))
    model <- board |> pins::pin_read("titanic-model")
    input <- data.frame(Sex = sex, Pclass = pclass)
    pred <- tidypredict::tidypredict_to_column(input, model)
    
    return(pred)
    
}

```

Executando a api

```{r eval=FALSE}
#| echo: true

plumber::pr_run(plumber::pr("R/plumber.R"), host = "0.0.0.0", port = 8080)
```

## `httr::` fazendo requisições

```{r eval=FALSE}
#| echo: true

"http://localhost:8080/predict" |> 
    httr::POST(body = list(sex = "female", pclass = "1st"), encode = "json") |>
    httr::content() |> 
    jsonlite::toJSON(pretty = TRUE,
                     auto_unbox = TRUE)
```

```
[
{
"Sex": "female",
"Pclass": "1st",
"fit": 0.9097
}
]
```

```{r eval=FALSE}
#| echo: true

"http://localhost:8080/predict" |> 
    httr::POST(body = list(sex = "male", pclass = "3rd"), encode = "json") |>
    httr::content() |> 
    jsonlite::toJSON(pretty = TRUE,
                     auto_unbox = TRUE)
```

```
[
{
"Sex": "male",
"Pclass": "3rd",
"fit": 0.082
}
] 
```

## `quantmod::` preços de ações

```{r eval=FALSE}
#| echo: true

dataStocks <- function(code, from, to) {
    
    stock <- 
        quantmod::getSymbols(code, 
                             src = "yahoo", 
                             from = from, 
                             to = to, 
                             auto.assign = FALSE)
    
    stock <-
        stock |>
        as.data.frame() |> 
        tibble::rownames_to_column(var = "Date") |> 
        dplyr::as_tibble() |> 
        dplyr::select(dplyr::matches("Date|Adjusted")) |> 
        dplyr::mutate(Date = Date |> lubridate::as_date()) |> 
        dplyr::rename(Adjusted = 2)
    
    return(stock)
    
}
```


## `shiny::` criando aplicações web

```{r eval=FALSE}
#| echo: true

library(shiny)

ui <- fluidPage(
    h1("Olá Shiny!")
    
)

server <- function(input, output, session) {
    
}

shinyApp(ui, server)
```

## `shiny::` app para obter preços de ações

:::: columns
::: {.column width="45%"}
![](resources/images/stock-price-app.png){width="80%"}
:::
::: {.column width="55%"}
```{r eval=FALSE}
#| echo: true

library(shiny)
library(shinythemes)

source("dataStocks.R")

start_value <- Sys.Date() - 31
end_value <- Sys.Date()

ui <- fluidPage(
    theme = shinytheme("sandstone"),
    navbarPage("STOCK PRICE"),
    sidebarLayout(
        sidebarPanel(
            textInput("code", "Stock code", value = "PETR4.SA"),
            dateRangeInput("date", "Date", 
                           start = start_value,
                           end = end_value)
        ),
        
        mainPanel(
            plotly::plotlyOutput("plot")
        )
    )
)

server <- function(input, output, session) {
    
    data_stock <- reactive({
        
        validate(
            need(input$code, "Please type a stock code!"),
            need(input$date[1], "Please choose a start date!"),
            need(input$date[2], "Please choose a end date!"))
        
        dataStocks(input$code, input$date[1], input$date[2])
    })
    
    output$plot <- plotly::renderPlotly({
        plotly::plot_ly(data_stock(), 
                        x = ~Date, 
                        y = ~Adjusted, 
                        color = "orange",
                        mode = "lines",
                        type = "scatter") |> 
            plotly::rangeslider()
    })
    
}

shinyApp(ui, server)
```
:::
::::

## R com `docker`

`Dockerfile`

```
FROM openwhisk/dockerskeleton
ARG NOT_CRAN=true
ARG ARROW_R_DEV=true
RUN apk update && apk add R R-dev R-doc \
build-base libsodium-dev autoconf automake bash \ 
cmake g++ gcc make libxml2-dev
RUN R -e "install.packages(c('jsonlite', 'tidypredict', 'yaml', \ 
'pins', 'paws.storage'), repos = 'http://cran.rstudio.com/')"
```

Build

```
podman build -t docker.io/th1460/titanic-classifier .
```

Push para um registry

```
podman login -u <user> -p <password> docker.io/th1460/titanic-classifier
podman push docker.io/th1460/titanic-classifier
```

## R e IBM Cloud Functions

:::: columns
::: {.column width="30%"}

> O serviço IBM Cloud Functions é uma plataforma de computação orientada a eventos, também conhecida como computação sem servidor ou como Function-as-a-Service (FaaS), que executa código em resposta a eventos ou chamadas diretas.

:::

::: {.column width="70%"}
`exec`

```
#!/bin/bash
# run R script
chmod +x script.R # turn executable
echo "$@" > input.json # set input
./script.R # run script
```

`script.R`

```
#!/usr/bin/env Rscript

readRenviron(".Renviron")

# load model
board <- pins::board_s3(bucket = Sys.getenv("COS_BUCKET"), 
region = Sys.getenv("COS_REGION"), 
access_key = Sys.getenv("COS_ACCESS_KEY_ID"),
secret_access_key = Sys.getenv("COS_SECRET_ACCESS_KEY"),
endpoint = Sys.getenv("COS_ENDPOINT"))

model <- pins::pin_read(board, "titanic-model")

# input
input <- jsonlite::fromJSON("input.json", flatten = FALSE)

# predict
pred <- tidypredict::tidypredict_to_column(as.data.frame(input), model)

# output
jsonlite::stream_out(pred, verbose = FALSE)
```
:::
::::

## Deploy do modelo na IBM Cloud Functions

:::: columns
::: {.column}
Login na IBM Cloud

```
ibmcloud login -sso
ibmcloud target --cf
```

Empacotar os arquivos para o deploy 

```
zip -r titanic.zip exec script.R .Renviron
```

Deploy

```
ibmcloud fn action create titanic-classifier titanic.zip \ 
--docker th1460/titanic-classifier --web true
```

Descobrindo qual é a url

```
ibmcloud fn action get titanic-classifier --url
```
:::
::: {.column}
Fazendo uma requisição na function

```{r eval=FALSE}
#| echo: true

results <- "<URL>.json" |> 
    httr::POST(body = list(Sex = "male", Pclass = "3rd"), 
               encode = "json") |> httr::content()

results[c("Sex", "Pclass", "fit")] |> 
    jsonlite::toJSON(pretty = TRUE,
                     auto_unbox = TRUE)
```

```
{
"Sex": "male",
"Pclass": "3rd",
"fit": 0.082
} 
```
:::
::::

## `rayshader::` mapas em 3D

```{r}
#| echo: true

# Load elevation data (https://th1460.github.io/posts/2022/08/mapa-3d-com-rayshader/)
elev_file <- "../data/rj.tif"
elev_img <- raster::raster(elev_file)
elev_matrix <- 
    matrix(raster::extract(elev_img, 
                           raster::extent(elev_img), 
                           buffer = 1000), 
           nrow = ncol(elev_img), 
           ncol = nrow(elev_img)
    )
```

```{r}
#| echo: true
#| output-location: column

# Define bounding box with longitude/latitude coordinates
bbox <- list(
    p2 = list(long = -43.2328217452992, lat = -22.99560928307949),
    p1 = list(long = -43.133008448808454, lat = -22.930329210944166)
)

leaflet::leaflet() |> 
    leaflet::addTiles() |> 
    leaflet::addRectangles(
        lng1 = bbox$p1$long, lat1 = bbox$p1$lat,
        lng2 = bbox$p2$long, lat2 = bbox$p2$lat,
        fillColor = "transparent"
    ) |> 
    leaflet::fitBounds(
        lng1 = bbox$p1$long, lat1 = bbox$p1$lat,
        lng2 = bbox$p2$long, lat2 = bbox$p2$lat,
    )
```

## `rayshader::` mapas em 3D

```{r}
#| echo: true
#| output-location: column

source("../src/define_image_size.R")
source("../src/get_arcgis_map_image.R")

image_size <- define_image_size(bbox, major_dim = 600)

# Fetch overlay image
overlay_file <- "../data/rj_overlay.png"

get_arcgis_map_image(bbox, map_type = "World_Imagery", file = overlay_file,
                     width = image_size$width, height = image_size$height, 
                     sr_bbox = 4326)

overlay_img <- png::readPNG(overlay_file)

# Calculate rayshader layers
watermap <- rayshader::detect_water(elev_matrix)

# Plot 2D with texture
elev_matrix |> 
    rayshader::sphere_shade(texture = "imhof4") |> 
    rayshader::add_water(watermap, color = "imhof4") |> 
    rayshader::add_overlay(overlay_img, alphalayer = 0.5) |> 
    rayshader::plot_map()
```

## `rayshader::` mapas em 3D

```{r, eval=FALSE}
#| echo: true

# Plot 3D
zscale <- 30
rgl::clear3d()
elev_matrix |> 
    rayshader::sphere_shade(texture = "imhof4") |> 
    rayshader::add_water(watermap, color = "imhof4") |> 
    rayshader::add_overlay(overlay_img, alphalayer = .8) |> 
    plot_3d(elev_matrix, zscale = zscale, windowsize = c(1500, 1200),
            water = TRUE, soliddepth = -max(elev_matrix)/zscale, wateralpha = 1,
            theta = 25, phi = 30, zoom = 0.65, fov = 60)
rayshader::render_snapshot("../data/rj-3d.png")
```

```{r}
magick::image_read("../data/rj-3d-editado.png")
```

## `purrr::` programação funcional

Fazendo loop com `for`

```{r}
#| echo: true

x <- NULL
for (i in 1:10) {
    x <- c(x, i^2)
}
x
```

Fazendo loop com `map_dbl`

```{r}
#| echo: true

1:10 |> 
    purrr::map_dbl(\(x) x^2)
```

## `keras::dataset_imdb()` mineração de texto

Estruturando a base de dados

```{r eval=FALSE}
#| echo: true

imdb <- keras::dataset_imdb(num_words = 10000)

set.seed(1984)
train_labels <- 
    dplyr::tibble(y = imdb$train$y, id = 1:25000) |>  
    dplyr::group_by(y) |> 
    dplyr::sample_n(size = 5000) |> 
    dplyr::ungroup()

train_data <- 
    train_labels |> 
    dplyr::pull(id) |> 
    purrr::map(~ imdb$train$x[[.x]])
```

## `keras::dataset_imdb_word_index()` mineração de texto

Decodificando o review

```{r eval=FALSE}
#| echo: true

word_index <- keras::dataset_imdb_word_index()
reverse_word_index <- names(word_index)
names(reverse_word_index) <- word_index

decoded_review <- function(x) {sapply(train_data[[x]], function(index) {
    word <- if (index >= 3) reverse_word_index[[as.character(index - 3)]]
    if (!is.null(word)) word else "?"
})}

data_decoded_review <- 
    seq(10000) |> 
    purrr::map(~ decoded_review(.x))

data_decoded_review <-
    tibble::enframe(data_decoded_review,
                    name = "index",
                    value = "words")

data_decoded_review <-
    data_decoded_review |> 
    dplyr::bind_cols(train_labels)

data_decoded_review |> 
    saveRDS("../data/data_decoded_review.RDS")

```

## `tidytext::` e `wordcloud::`

```{r}
#| echo: true
#| output-location: column
#| fig.width: 12
#| fig.height: 11

data_decoded_review <- readRDS("../data/data_decoded_review.RDS")

set.seed(1984)
data_decoded_review |> 
    tidyr::unnest(words) |> 
    dplyr::anti_join(tidytext::stop_words, by = c("words" = "word")) |> 
    dplyr::filter(! words %in% c("?", "br", "1", "2", 
                                 "3", "4", "7", "8", 
                                 "9", "10")) |> 
    dplyr::count(words, y, sort = TRUE) |> 
    dplyr::mutate(y = ifelse(y == 0, "Negativo", "Positivo")) |> 
    reshape2::acast(words ~ y, value.var = "n", fill = 0) |> 
    wordcloud::comparison.cloud(colors = c("#E24664", "#1D6CE6"),
                                max.words = 200,
                                title.size = 4,
                                scale = c(5, 2))
```

## `tensorflow::` deep learning

### Criando um modelo para avaliar se um review de filme é positivo ou negativo

```{r eval=FALSE}
#| echo: true

imdb <- keras::dataset_imdb(num_words = 10000)

set.seed(1984)
train_labels <- 
    dplyr::tibble(y = imdb$train$y, id = 1:25000) |>  
    dplyr::group_by(y) |> 
    dplyr::sample_n(size = 5000) |> 
    dplyr::ungroup()

train_data <- 
    train_labels |> 
    dplyr::pull(id) |> 
    purrr::map(~ imdb$train$x[[.x]])

saveRDS(list(data = train_data, 
             labels = train_labels), "../data/train_imdb.RDS")

test_labels <- 
    dplyr::tibble(y = imdb$test$y, id = 1:25000) |>  
    dplyr::group_by(y) |> 
    dplyr::sample_n(size = 5000) |> 
    dplyr::ungroup()

test_data <- 
    test_labels |> 
    dplyr::pull(id) |> 
    purrr::map(~ imdb$test$x[[.x]])

saveRDS(list(data = test_data, 
             labels = test_labels), "../data/test_imdb.RDS")
```

## `tensorflow::` deep learning

### Função para Tranformando os reviews (inputs) em um vetor de 0s e 1s

```{r}
#| echo: true

vectorize_sequences <- function(sequences, dimension = 10000) {
    # Creates an all-zero matrix of shape (length(sequences), dimension)
    results <- matrix(0, nrow = length(sequences), ncol = dimension) 
    for (i in 1:length(sequences))
        # Sets specific indices of results[i] to 1s
        results[i, sequences[[i]]] <- 1 
    results
}
```

```{r eval=FALSE}
#| echo: true

train_imdb <- readRDS("../data/train_imdb.RDS")
test_imdb <- readRDS("../data/test_imdb.RDS")

# tranformando os reviews (inputs) em um vetor de 0s e 1s
x_train <- vectorize_sequences(train_imdb$data)
x_test <- vectorize_sequences(test_imdb$data)
# transformando os outpus em numérico
y_train <- as.numeric(train_imdb$labels |> pull(y))
y_test <- as.numeric(train_imdb$labels |> pull(y))
```

## `tensorflow::` deep learning

### Estrutura da rede

```{r}
#| echo: true
DiagrammeR::grViz("resources/images/mlp.gv")
```

## `tensorflow::` deep learning

### Construindo a rede

```{r eval=FALSE}
#| echo: true

model <- keras::keras_model_sequential() |> 
    keras::layer_dense(units = 10, activation = "relu", input_shape = c(10000)) |> 
    keras::layer_dense(units = 10, activation = "relu") |> 
    keras::layer_dense(units = 1, activation = "sigmoid")
```

### Treinamento

```{r eval=FALSE}
#| echo: true

model |> keras::compile(
    optimizer = "rmsprop",
    loss = "binary_crossentropy",
    metrics = c("accuracy")
)
model |> keras::fit(x_train, y_train, epochs = 4, batch_size = 512)
```

### Avaliando o modelo

```{r eval=FALSE}
#| echo: true

(results <- model |> keras::evaluate(x_test, y_test))
```

```
    loss accuracy 
0.350753 0.870200 
```

### Salvando o modelo

```{r eval=FALSE}
#| echo: true

model |> keras::export_savedmodel("../data/savedmodel")
```

## `tensorflow::` deep learning

### Classificando um review

```{r}
#| echo: true
#| output-location: column

# codificação
coded_review <- function(review){
    
    word_index <- keras::dataset_imdb_word_index()
    reverse_word_index <- unlist(word_index)
    
    words <-
        (review |> 
             tolower() |> 
             strsplit(split = " "))[[1]] |> 
        gsub("[!,:]", "", x = _)
    
    index <- reverse_word_index[names(reverse_word_index) %in% words]
    
    return(list(as.numeric(index) + 3))
    
}

(exemplo1 <- 
  "The film is best, great, beautiful and wonderful!" |>  
  coded_review()) |> str()

# vetorização
(exemplo_vector1 <- vectorize_sequences(exemplo1)) |> str()

# classificação
exemplo_vector1 |> 
  tfdeploy::predict_savedmodel("../data/savedmodel")

```

## Problema de otimização de rota de frota

### Mapa do centro de Campinas SP

```{r}
#| echo: true
#| output-location: column

bbox <- list(
    p1 = list(long = -47.0708, lat = -22.9075),
    p2 = list(long = -47.0348, lat = -22.8804)
)

leaflet::leaflet() |> 
    leaflet::addTiles()|> 
    leaflet::addRectangles(
        lng1 = bbox$p1$long, lat1 = bbox$p1$lat,
        lng2 = bbox$p2$long, lat2 = bbox$p2$lat,
        fillColor = "transparent"
    ) |> 
    leaflet::fitBounds(
        lng1 = bbox$p1$long, lat1 = bbox$p1$lat,
        lng2 = bbox$p2$long, lat2 = bbox$p2$lat,
    )
```

## Vias como linhas baseado no `osmdata`

```{r}
#| echo: true
#| output-location: column
#| fig-height: 6
#| fig-width: 6

if (!exists("has_internet_via_proxy")) {
    assign("has_internet_via_proxy", TRUE, environment(curl::has_internet))   
}

# Consulta
query <- osmdata::opq(bbox =  unlist(bbox)) |> 
    osmdata::add_osm_feature(key = "highway") |> 
    osmdata::osmdata_sf()

# Retornando linhas
query_lines <- query$osm_lines

# Visualizando o objeto
query_lines |> 
    sf::st_geometry() |> 
    ggplot2::ggplot() +
    ggplot2::geom_sf(colour = "gray") +
    ggplot2::theme_minimal() +
    ggplot2::theme(text = element_text(size = 15))

```

## Convertendo as vias em um objeto `sfnetwork`

```{r}
#| echo: true
#| output-location: column
#| fig-height: 6
#| fig-width: 6

poly_to_lines <- sf::st_cast(query$osm_polygons, "LINESTRING")

query_lines <- dplyr::bind_rows(query_lines, poly_to_lines)

highway_net <- sfnetworks::as_sfnetwork(query_lines, directed = FALSE)

# Remove nodes that have only two edges connected
highway_simple <- tidygraph::convert(highway_net, sfnetworks::to_spatial_smooth)

# Filter connected components
connected_net <- highway_simple |> 
    tidygraph::activate("nodes") |> 
    dplyr::filter(tidygraph::group_components() == 1)

# Plot
ggplot2::ggplot() +
    ggplot2::geom_sf(data = sf::st_as_sf(connected_net, "edges"), col = "gray") +
    ggplot2::geom_sf(data = sf::st_as_sf(connected_net, "nodes"), col = "gray") +
    ggplot2::theme_minimal() +
    ggplot2::theme(text = element_text(size = 15))

```

## Problema de otimização de rota de frota

São 3 veículos e preciso selecionar a melhor rota. Eles devem sair e retornar a um depósito.

```{r}
#| echo: true
#| output-location: column
#| fig-height: 7
#| fig-width: 7

set.seed(711)
index_visits <- sample(1:603, 9)
visits <- connected_net |> 
    tidygraph::activate("nodes") |> 
    dplyr::slice(index_visits)

depot <- connected_net |> 
    tidygraph::activate("nodes") |> 
    dplyr::slice(23)

ggplot2::ggplot() + 
    ggplot2::geom_sf(data = sf::st_as_sf(connected_net, "edges"), col = "gray") +
    ggplot2::geom_sf(data = sf::st_as_sf(connected_net, "nodes"), col = "gray") +
    ggplot2::theme_minimal() +
    ggplot2::theme(text = element_text(size = 15)) +
    ggplot2::geom_sf(data = sf::st_as_sf(visits, "nodes"), 
                     ggplot2::aes(col = "Visits"), size = 6) +
    ggplot2::geom_sf(data = sf::st_as_sf(depot, "nodes"), 
                     ggplot2::aes(col = "Depot"), size = 6) +
    ggplot2::labs(x = "", y = "", colour = "") +
    ggplot2::scale_color_manual(values = c("Visits" = "blue", 
                                           "Depot" = "orange"))

```

## Problema de otimização de rota de frota

Cálculo da matriz de custo

```{r}
#| echo: true
#| output-location: column

connected_net <- 
    connected_net |> 
    tidygraph::activate("edges") |> 
    dplyr::mutate(weight = sfnetworks::edge_length())

cost_matrix_route_problem <- 
    sfnetworks::st_network_cost(connected_net, 
                                from = c(23, index_visits), 
                                to = c(23, index_visits), 
                                weights = "weight")

mode(cost_matrix_route_problem) <- "integer"

cost_matrix_route_problem
```

## Problema de otimização de rota de frota utilizando `ortools` e `reticulate::`

:::: columns
::: {.column}
```{r eval=FALSE}
#| echo: true

reticulate::use_virtualenv("../venv", required = TRUE)
constraint_solver <- reticulate::import("ortools.constraint_solver")

# Create data
create_data_model <- function() {
    
    data <- reticulate::dict()
    data['distance_matrix'] <- cost_matrix_route_problem
    data['distance_matrix'] <-  data['distance_matrix']$tolist()
    data['num_vehicles'] <- 3L
    data['depot'] <- 0L
    
    return(data)
    
}

# Create the routing model
data <- create_data_model()
manager <- constraint_solver$pywrapcp$RoutingIndexManager(
    length(data['distance_matrix']),
    data['num_vehicles'],
    data['depot']
)

routing <- constraint_solver$pywrapcp$RoutingModel(manager)

# Create the distance callback
distance_callback <- function(from_index, to_index) {
    
    from_node <- manager$IndexToNode(from_index)
    to_node <- manager$IndexToNode(to_index)
    
    return(data['distance_matrix'][from_node][to_node])
    
}

transit_callback_index <- routing$RegisterTransitCallback(distance_callback)

# Set the cost of travel
routing$SetArcCostEvaluatorOfAllVehicles(transit_callback_index)

# Add a distance dimension
dimension_name <- 'Distance'
routing$AddDimension(
    transit_callback_index,
    0L,  # no slack
    15000L,  # vehicle maximum travel distance
    TRUE,  # start cumul to zero
    dimension_name)

distance_dimension <- routing$GetDimensionOrDie(dimension_name)
distance_dimension$SetGlobalSpanCostCoefficient(100L)

# Set search parameters
search_parameters <- constraint_solver$pywrapcp$DefaultRoutingSearchParameters()
search_parameters$first_solution_strategy = (
    constraint_solver$routing_enums_pb2$FirstSolutionStrategy$PATH_CHEAPEST_ARC)

solution <- routing$SolveWithParameters(search_parameters)

# Save routes to a list or array
get_routes <- function(solution, routing, manager) {
    
    routes <- list()
    
    for (route_nbr in seq(0, routing$vehicles() - 1)) {
        
        index <- routing$Start(route_nbr)
        route <- c(manager$IndexToNode(index))
        
        while(!routing$IsEnd(index)) {
            
            index <- solution$Value(routing$NextVar(index))
            route <- append(route, manager$IndexToNode(index))
            
        }
        
        routes[[route_nbr + 1]] <- route
        names(routes)[route_nbr + 1] <- route_nbr
        
    }
    
    return(routes)
    
}

(routes <- get_routes(solution, routing, manager))
```
:::
::: {.column}
```
$`0`
[1] 0 6 9 0

$`1`
[1] 0 3 2 5 0

$`2`
[1] 0 7 1 8 4 0
```
:::
::::

## Problema de otimização de rota de frota

```{r}
#| echo: true
#| output-location: column
#| fig-height: 8
#| fig-width: 8

real_routes <- lapply(readRDS("../data/routes.RDS"), \(x) c(23, index_visits)[x + 1])

vrp_paths <- function(x) {
    
    results <- list()
    for (i in seq(length(x))) {
        
        from_idxs <- unique(x[[i]])
        to_idxs <- c(from_idxs[2:length(from_idxs)], from_idxs[1])
        
        paths <- mapply(sfnetworks::st_network_paths,
                        from = from_idxs,
                        to = to_idxs,
                        MoreArgs = list(x = connected_net, weights = "weight"))    
        
        results[[i]] <- paths
    }
    
    return(results)
    
}

# Plot results
vrp_paths_fun <- function(x, index, vehicle) {
    
    vrp_edge_paths <- connected_net |> 
        tidygraph::activate("edges") |> 
        dplyr::slice(vrp_paths(x)[[vehicle]][, index]$edge_paths |> unlist())
    
    ggplot2::geom_sf(data = sf::st_as_sf(vrp_edge_paths, "edges"),
                     ggplot2::aes(col = as.character(vehicle)), lwd = 1.3)
    
}

p <- ggplot2::ggplot() +
    ggplot2::geom_sf(data = sf::st_as_sf(connected_net, "edges"), col = "gray") +
    ggplot2::geom_sf(data = sf::st_as_sf(connected_net, "nodes"), col = "gray") +
    ggplot2::theme_minimal()

for (j in seq(length(real_routes))) {
    for (i in seq(length(real_routes[[j]]) - 1)) {
        p <- p + vrp_paths_fun(real_routes, i, j)   
    }
}

p <- p + ggplot2::geom_sf(data = sf::st_as_sf(visits, "nodes"), 
                     ggplot2::aes(col = "Visits"), size = 6) +
    ggplot2::geom_sf(data = sf::st_as_sf(depot, "nodes"), 
                     ggplot2::aes(col = "Depot"), size = 6) +
    ggplot2::labs(x = "", y = "", colour = "") +
    ggplot2::theme(text = element_text(size = 15))

plotly::ggplotly(p, tooltip = "") |> 
    plotly::layout(modebar = list(orientation = "v"))

```


## Linguagem R testada com outros recursos da IBM Cloud

<ul>
<li>Kubernetes/Openshift</li>
<li>Code Engine</li>
<li>Cloud Pak for Data</li>
<li>App ID</li>
</ul>

## Projetos pessoais com R

<ul>
<li>Autenticação de aplicações shiny com App ID: [github.com/IBM/AppIdR](https://github.com/IBM/AppIdR)</li>
<li>Autenticação de aplicações shiny com W3ID: [github.ibm.com/Thiago-Pires/w3idr](https://github.ibm.com/Thiago-Pires/w3idr)</li>
<li>Carbon Design System em Shiny apps: [github.com/th1460/shiny-carbon-web](https://github.com/th1460/shiny-carbon-web)</li>
</ul>